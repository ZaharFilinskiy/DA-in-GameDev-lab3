# АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ [in GameDev]
Отчет по лабораторной работе #3 выполнил(а):
- Филинский Захар Евгеньевич
- РИ211121
Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | # | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

Структура отчета

- Данные о работе: название работы, фио, группа, выполненные задания.
- Цель работы.
- Задание 1.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 2.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 3.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Выводы.
- ✨Magic ✨

## Цель работы
Познакомиться с программными средствами для создания системы машинного обучения и ее интеграции в Unity.

## Задание 1
### Реализовать систему машинного обучения в связке Python - Google-Sheets – Unity. При выполнении задания можно использовать видео-материалы и исходные данные, предоставленные преподавателями курса.

Ход работы: Работа над заданием началась с повторения настройки программ, с видео из методических указаний, но по мере углубления в процесс пришлось читать доп.информацию и находить ответы самому. Поэтому, лишь проанализировав действия, удалось усвоить информацию.

-Выполнив каждый ход работы, от связи Unity с MlAgent:

![2022-11-05_19-37-17](https://user-images.githubusercontent.com/114186148/200125153-37d1b9e3-8f50-4232-bbf1-8a41f01c3565.png)

-До обучения объекта через anaconda promt и добавления обьектов:

![2022-11-05_19-43-17](https://user-images.githubusercontent.com/114186148/200125417-d779bfb0-976c-43db-8aff-03ff13a304d2.png)

-Мне удалось понять принцип работы с обучаемой системой в unity, поближе познакомиться с данным способом работы в игровой индустрии и собственноручно, много раз, "натренировать" нейросеть:

![2022-11-05_19-48-33](https://user-images.githubusercontent.com/114186148/200125677-c18b3b80-bd08-42fc-95c6-b41296a7e6ed.png)

![2022-11-05_19-49-43](https://user-images.githubusercontent.com/114186148/200125748-4429dfa3-ba72-4ef0-881c-755b5663d2d8.png)

-Также для себя я подметил некоторые интересные моменты, которые влияют на работу всей системы. Оказывается для полноценного машинного обучения важно иметь совместимые версии программ, работающих между собой, уделять внимание правильной настройке компонентов, таких как обьекты и програмный код и больше читать. Потому-что знаний мало, а сделать хочется много.

## Задание 2
### Подробно опишите каждую строку файла конфигурации нейронной сети, доступного в папке с файлами проекта по ссылке. Самостоятельно найдите информацию о компонентах Decision Requester, Behavior Parameters, добавленных на сфере.

trainer_type – задаёт тип используемого для обучения тренажёра (PPO или SAC)

batch_size – Количество опытов на каждой итерации градиентного спуска.

buffer_size – Количество опыта, которое нужно собрать для обновления итерации или её изучения.

learning_rate – изменение скорости обучения модели с течением времени.

beta – исследование случайных (более или менее) пространств действий.

epsilon – параметр влияющий на быстроту развития (ускорения работы) системы с каждой итерацией.

lambd – параметр оценивающий совпадение стоимости вознаграждения обучений между собой (что то вроде погрешности), чем стабильнее значения. тем быстрее идёт процесс.

num_epoch – Количество проходов через буфер опыта при выполнении оптимизации градиентного спуска. (чем больше, тем быстрее итерация и наоборот)

learning_rate_schedule – скорость обучения, которая в нашем случае уменьшается линейно до нуля.

normalize – отвечает за то будут ли нормироваться (усредняться) входные данные (в нашем случае -нет)

hidden_units – количество значений в подключенном слое нейронной сети (то есть чем больше будет поток входных данных, тем больше нужно ввести значения параметра и наоборот).

num_layers – Количество скрытых слоев в нейронной сети. Число слоёв, принимающих на себя работу, чем сложнее задача, тем больше нужно слоёв.

gamma – Параметр, отвечающий за то, насколько далеко вперёд должен думать о вознаграждении агент. Должен быть в состоянии подготовиться – выделить объём места и т.д. 

strength – Коэффициент, на который умножается вознаграждение, предоставляемое средой. Благодаря этому параметру можно увеличивать или уменьшать количество поступаемой валюты.

max_steps – Общее количество шагов-действий, которые должны быть выполнены в среде до завершения процесса обучения.

time_horizon – Сколько опыта нужно собрать перед тем, как добавить в буфер. Также используется, как среднее значение для общего ожидаемого вознаграждения.

summary_freq – Количество опыта, которое необходимо собрать перед созданием и отображением статистики обучения.

hyperparameters – Группировка параметров, отвечающих за управления процессом обучения

network_settings – Группировка параметров, отвечающих за обучение сети.

reward_signals – Раздел позволяющий задавать настройки как для внешних, так и для внутренних сигналов вознаграждения.

extrinsic – внешний сигнал из reward_signals

Decision Requester это компонент автоматически запрашивающий решение с постоянным интервалом времени. Тоесть он отвечает за принятие решения в цикле: наблюдение-принятия решения-действие-вознаграждение.

Behavior Parameters -это компонент выполняющий функции настройки поведения агента (генерирует объекты и их свойства согласно заданным параметрам).

## Задание 3
### Доработайте сцену и обучите ML-Agent таким образом, чтобы шар перемещался между двумя кубами разного цвета. Кубы должны, как и в первом задании, случайно изменять координаты на плоскости.


## Выводы

Выполнив лабораторную работу невольно задумываешься о игровом балансе. Системе при которой достигается равновесие меджду различными игровыми объектами или аспектами. В нашем случае свзанном с валютой, которую получает игрок при действиях. Здесь как и в реальном мире существуют такие понятия как инфляция и т.п, следовательно важно следить за оборотом монет (чтобы их был не слишком много и не слишком мало). Также можно переводить часть монет в более высокую по ценность валюту. Но постоянно следить за этим сложно. Тогда на помощь может придти машинное обучение - при использовании модели с тестированием и обучением (где заранее можно задать необходимые параметры), можно не только заранее выявить потенциальные источники дисбаланса, но и быстро внести изменения, позволяющие смягчить их воздействие. 

## Powered by

**BigDigital Team: Denisov | Fadeev | Panov**
